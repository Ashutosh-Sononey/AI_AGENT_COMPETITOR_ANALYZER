# IntelTracker/agent.py (Version 12.0 - Production Ready)
# Comprehensive competitive intelligence with email, scheduling, and advanced analysis

import os
import json
import hashlib
import requests
import feedparser
import re
import schedule
import time
import logging
import smtplib
import csv
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from typing import Optional, TYPE_CHECKING, List, Dict, Any, Tuple
from pathlib import Path
from dataclasses import dataclass, asdict, field
from enum import Enum
from collections import defaultdict
import threading

# ADK Imports
from google.adk.agents import LlmAgent
from google.adk.tools import FunctionTool
from google.adk.sessions import Session

if TYPE_CHECKING:
    from google.adk.tools import ToolContext

# Gemini API
try:
    import google.generativeai as genai
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
    if GEMINI_API_KEY:
        genai.configure(api_key=GEMINI_API_KEY)
        GEMINI_AVAILABLE = True
    else:
        GEMINI_AVAILABLE = False
except ImportError:
    GEMINI_AVAILABLE = False

# --- Configuration ---
DATA_DIR = Path("./data")
DATA_DIR.mkdir(exist_ok=True)
COMPETITORS_FILE = DATA_DIR / "competitors.json"
CACHE_FILE = DATA_DIR / "content_cache.json"
REPORTS_DIR = DATA_DIR / "reports"
REPORTS_DIR.mkdir(exist_ok=True)
CONFIG_FILE = DATA_DIR / "config.json"
LOG_FILE = DATA_DIR / "intel_tracker.log"
EXPORTS_DIR = DATA_DIR / "exports"
EXPORTS_DIR.mkdir(exist_ok=True)
TRENDS_FILE = DATA_DIR / "trends.json"

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Suppress noisy loggers
logging.getLogger('urllib3').setLevel(logging.WARNING)
logging.getLogger('requests').setLevel(logging.WARNING)

# --- Data Models ---
class ImpactLevel(Enum):
    CRITICAL = "Critical"
    HIGH = "High"
    MEDIUM = "Medium"
    LOW = "Low"

class SourceType(Enum):
    WEBSITE = "website"
    RSS = "rss"
    SOCIAL_MEDIA = "social_media"
    PRESS_RELEASE = "press_release"
    API = "api"

@dataclass
class CompetitorConfig:
    name: str
    website_url: Optional[str] = None
    rss_feed_url: Optional[str] = None
    twitter_handle: Optional[str] = None
    linkedin_url: Optional[str] = None
    press_release_url: Optional[str] = None
    category: str = "Unknown"
    priority: str = "Medium"
    enabled: bool = True
    tags: List[str] = field(default_factory=list)
    added_at: str = None
    last_checked: Optional[str] = None
    
    def __post_init__(self):
        if not self.added_at:
            self.added_at = datetime.now().isoformat()

@dataclass
class Update:
    title: str
    content: str
    source_type: str
    url: str
    date: Optional[str] = None
    impact: str = "Medium"
    detected_at: str = field(default_factory=lambda: datetime.now().isoformat())
    
@dataclass
class Analysis:
    summary: str
    key_features: List[str]
    impact: str
    action: str
    competitor_name: str
    threat_level: str = "Medium"
    opportunities: List[str] = field(default_factory=list)
    strategic_implications: Optional[str] = None
    confidence_score: float = 0.8
    
@dataclass
class MonitoringConfig:
    check_interval_hours: int = 24
    enable_scheduling: bool = False
    notification_email: Optional[str] = None
    smtp_server: str = "smtp.gmail.com"
    smtp_port: int = 587
    smtp_username: Optional[str] = None
    smtp_password: Optional[str] = None
    max_updates_per_competitor: int = 5
    min_impact_level: str = "Low"
    digest_format: str = "detailed"
    notify_on_critical: bool = True
    notify_on_high: bool = True
    export_after_check: bool = False
    rate_limit_seconds: int = 2

# --- Email Notification System ---
class EmailNotifier:
    """Send email notifications for important updates."""
    
    def __init__(self, config: MonitoringConfig):
        self.config = config
    
    def send_digest(self, report: str, findings_count: int) -> bool:
        """Send intelligence digest via email."""
        if not self.config.notification_email:
            return False
        
        try:
            msg = MIMEMultipart('alternative')
            msg['Subject'] = f"üéØ IntelTracker: {findings_count} Competitor Updates"
            msg['From'] = self.config.smtp_username or "IntelTracker"
            msg['To'] = self.config.notification_email
            
            # Create plain text and HTML versions
            text_content = self._strip_markdown(report)
            html_content = self._markdown_to_html(report)
            
            msg.attach(MIMEText(text_content, 'plain'))
            msg.attach(MIMEText(html_content, 'html'))
            
            # Send email
            with smtplib.SMTP(self.config.smtp_server, self.config.smtp_port) as server:
                server.starttls()
                if self.config.smtp_username and self.config.smtp_password:
                    server.login(self.config.smtp_username, self.config.smtp_password)
                server.send_message(msg)
            
            logger.info(f"Email sent to {self.config.notification_email}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to send email: {e}")
            return False
    
    def send_alert(self, competitor_name: str, impact: str, summary: str) -> bool:
        """Send immediate alert for critical/high impact updates."""
        if not self.config.notification_email:
            return False
        
        # Check if we should send alert for this impact level
        if impact == "Critical" and not self.config.notify_on_critical:
            return False
        if impact == "High" and not self.config.notify_on_high:
            return False
        if impact not in ["Critical", "High"]:
            return False
        
        try:
            msg = MIMEText(
                f"‚ö†Ô∏è {impact} Impact Update Detected\n\n"
                f"Competitor: {competitor_name}\n"
                f"Summary: {summary}\n\n"
                f"Check IntelTracker for full details."
            )
            msg['Subject'] = f"üö® {impact} Alert: {competitor_name}"
            msg['From'] = self.config.smtp_username or "IntelTracker"
            msg['To'] = self.config.notification_email
            
            with smtplib.SMTP(self.config.smtp_server, self.config.smtp_port) as server:
                server.starttls()
                if self.config.smtp_username and self.config.smtp_password:
                    server.login(self.config.smtp_username, self.config.smtp_password)
                server.send_message(msg)
            
            logger.info(f"Alert sent for {competitor_name}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to send alert: {e}")
            return False
    
    @staticmethod
    def _strip_markdown(text: str) -> str:
        """Convert markdown to plain text."""
        text = re.sub(r'#{1,6}\s', '', text)
        text = re.sub(r'\*\*([^\*]+)\*\*', r'\1', text)
        text = re.sub(r'\*([^\*]+)\*', r'\1', text)
        text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)
        return text
    
    @staticmethod
    def _markdown_to_html(text: str) -> str:
        """Basic markdown to HTML conversion."""
        html = f"<html><body style='font-family: Arial, sans-serif;'>{text}</body></html>"
        html = re.sub(r'#{1,6}\s(.+)', r'<h2>\1</h2>', html)
        html = re.sub(r'\*\*([^\*]+)\*\*', r'<strong>\1</strong>', html)
        html = re.sub(r'\*([^\*]+)\*', r'<em>\1</em>', html)
        html = re.sub(r'\[([^\]]+)\]\(([^\)]+)\)', r'<a href="\2">\1</a>', html)
        html = html.replace('\n\n', '<br><br>')
        return html

# --- Configuration Management ---
def load_config() -> MonitoringConfig:
    """Load monitoring configuration."""
    if CONFIG_FILE.exists():
        try:
            with open(CONFIG_FILE, 'r') as f:
                data = json.load(f)
                return MonitoringConfig(**data)
        except Exception as e:
            logger.warning(f"Failed to load config: {e}. Using defaults.")
    return MonitoringConfig()

def save_config(config: MonitoringConfig) -> None:
    """Save monitoring configuration."""
    with open(CONFIG_FILE, 'w') as f:
        json.dump(asdict(config), f, indent=2)

# --- Storage Functions ---
def load_competitors() -> List[CompetitorConfig]:
    """Load competitor configurations."""
    if COMPETITORS_FILE.exists():
        try:
            with open(COMPETITORS_FILE, 'r') as f:
                data = json.load(f)
                return [CompetitorConfig(**comp) for comp in data]
        except Exception as e:
            logger.error(f"Failed to load competitors: {e}")
            return _get_default_competitors()
    return _get_default_competitors()

def _get_default_competitors() -> List[CompetitorConfig]:
    """Get default competitor list."""
    defaults = [
        CompetitorConfig(
            name="Notion",
            website_url="https://www.notion.so/releases",
            rss_feed_url="https://www.notion.so/releases/rss",
            category="Productivity",
            priority="High",
            tags=["productivity", "collaboration"]
        ),
        CompetitorConfig(
            name="Linear",
            website_url="https://linear.app/changelog",
            category="Project Management",
            priority="High",
            tags=["project-management", "development"]
        ),
        CompetitorConfig(
            name="Airtable",
            website_url="https://www.airtable.com/whatsnew",
            category="Database",
            priority="Medium",
            tags=["database", "no-code"]
        ),
    ]
    save_competitors(defaults)
    return defaults

def save_competitors(competitors: List[CompetitorConfig]) -> None:
    """Save competitor configurations."""
    with open(COMPETITORS_FILE, 'w') as f:
        json.dump([asdict(comp) for comp in competitors], f, indent=2)

def load_cache() -> Dict[str, Any]:
    """Load content cache."""
    if CACHE_FILE.exists():
        try:
            with open(CACHE_FILE, 'r') as f:
                return json.load(f)
        except:
            return {}
    return {}

def save_cache(cache: Dict[str, Any]) -> None:
    """Save content cache."""
    with open(CACHE_FILE, 'w') as f:
        json.dump(cache, f, indent=2)

def load_trends() -> Dict[str, Any]:
    """Load trend analysis data."""
    if TRENDS_FILE.exists():
        try:
            with open(TRENDS_FILE, 'r') as f:
                return json.load(f)
        except:
            return {}
    return {}

def save_trends(trends: Dict[str, Any]) -> None:
    """Save trend analysis data."""
    with open(TRENDS_FILE, 'w') as f:
        json.dump(trends, f, indent=2)

# --- Enhanced Content Extraction ---
class ContentExtractor:
    """Enhanced content extraction with multiple strategies."""
    
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    @staticmethod
    def extract_from_rss(url: str) -> List[Update]:
        """Extract updates from RSS feed."""
        try:
            feed = feedparser.parse(url)
            if not feed.entries:
                return []
            
            updates = []
            for entry in feed.entries[:10]:
                title = entry.get('title', 'Untitled')
                summary = entry.get('summary', entry.get('description', ''))
                
                if summary:
                    summary = BeautifulSoup(summary, 'html.parser').get_text(strip=True)
                
                updates.append(Update(
                    title=title,
                    content=summary[:800] if summary else title,
                    source_type=SourceType.RSS.value,
                    url=entry.get('link', url),
                    date=entry.get('published', None)
                ))
            
            logger.debug(f"Extracted {len(updates)} updates from RSS: {url}")
            return updates
            
        except Exception as e:
            logger.warning(f"RSS extraction failed for {url}: {e}")
            return []
    
    @staticmethod
    def extract_from_website(url: str) -> List[Update]:
        """Extract updates from website with improved cleaning."""
        try:
            response = requests.get(
                url, 
                timeout=15, 
                headers=ContentExtractor.HEADERS,
                allow_redirects=True
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            original_size = len(soup.get_text())
            
            soup = ContentExtractor._clean_soup(soup)
            cleaned_size = len(soup.get_text())
            
            logger.debug(f"Page cleaned: {original_size//1000}kb -> {cleaned_size//1000}kb")
            
            updates = ContentExtractor._extract_structured_content(soup, url)
            logger.debug(f"Extracted {len(updates)} updates from website: {url}")
            
            return updates
            
        except requests.Timeout:
            logger.warning(f"Timeout fetching {url}")
            return []
        except requests.RequestException as e:
            logger.warning(f"Website extraction failed for {url}: {e}")
            return []
        except Exception as e:
            logger.error(f"Unexpected error extracting from {url}: {e}")
            return []
    
    @staticmethod
    def _clean_soup(soup):
        """Remove unnecessary elements."""
        # Remove scripts, styles, nav, etc.
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 
                        'iframe', 'noscript', 'svg', 'button', 'input', 'select']):
            tag.decompose()
        
        # Remove elements with common noise patterns
        noise_patterns = ['cookie', 'consent', 'popup', 'modal', 'banner',
                         'newsletter', 'subscribe', 'chat', 'widget', 'ad',
                         'advertisement', 'tracking', 'analytics', 'social-share']
        
        for pattern in noise_patterns:
            for tag in soup.find_all(class_=lambda c: c and pattern in str(c).lower()):
                tag.decompose()
            for tag in soup.find_all(id=lambda i: i and pattern in str(i).lower()):
                tag.decompose()
        
        # Remove by role
        for tag in soup.find_all(attrs={'role': True}):
            if tag.get('role') in ['navigation', 'banner', 'complementary', 'contentinfo']:
                tag.decompose()
        
        return soup
    
    @staticmethod
    def _extract_structured_content(soup, url: str) -> List[Update]:
        """Extract structured content with multiple strategies."""
        updates = []
        
        # Find main content area
        main_content = (
            soup.find('main') or 
            soup.find('article') or 
            soup.find(id=lambda i: i and 'content' in str(i).lower()) or
            soup
        )
        
        # Strategy 1: Find article/section containers
        containers = main_content.find_all(
            ['article', 'section', 'div'],
            class_=lambda c: c and any(x in str(c).lower() 
                                      for x in ['post', 'update', 'release', 'item', 'entry', 'card', 'news'])
        )
        
        for container in containers[:15]:
            update = ContentExtractor._extract_from_container(container, url)
            if update and not any(u.title == update.title for u in updates):
                updates.append(update)
        
        # Strategy 2: Use headings if few updates found
        if len(updates) < 3:
            for heading in main_content.find_all(['h1', 'h2', 'h3'], limit=15):
                update = ContentExtractor._extract_from_heading(heading, url)
                if update and not any(u.title == update.title for u in updates):
                    updates.append(update)
        
        # Strategy 3: List items
        if len(updates) < 2:
            for ul in main_content.find_all('ul', limit=5):
                for li in ul.find_all('li', limit=10):
                    update = ContentExtractor._extract_from_list_item(li, url)
                    if update and not any(u.title == update.title for u in updates):
                        updates.append(update)
        
        return updates[:15]
    
    @staticmethod
    def _extract_from_container(container, url: str) -> Optional[Update]:
        """Extract update from a container element."""
        title_elem = container.find(['h1', 'h2', 'h3', 'h4', 'a'])
        if not title_elem:
            return None
        
        title = title_elem.get_text(strip=True)
        if not ContentExtractor._is_valid_content(title):
            return None
        
        # Get date
        date = None
        date_elem = container.find(['time', 'span'], 
                                   class_=lambda c: c and 'date' in str(c).lower())
        if date_elem:
            date = date_elem.get_text(strip=True)
        elif container.find('time'):
            date_elem = container.find('time')
            date = date_elem.get('datetime') or date_elem.get_text(strip=True)
        
        # Get content
        content_parts = []
        for p in container.find_all(['p', 'li'], limit=8):
            text = p.get_text(strip=True)
            if len(text) > 20 and ContentExtractor._is_valid_content(text):
                content_parts.append(text)
        
        content = ' '.join(content_parts)[:1000]
        
        if not content or len(content) < 30:
            content = title
        
        return Update(
            title=title[:250],
            content=content,
            source_type=SourceType.WEBSITE.value,
            url=url,
            date=date
        )
    
    @staticmethod
    def _extract_from_heading(heading, url: str) -> Optional[Update]:
        """Extract update from heading and following content."""
        title = heading.get_text(strip=True)
        if not ContentExtractor._is_valid_content(title) or len(title) < 10:
            return None
        
        # Get date from nearby time element
        date = None
        parent = heading.parent
        if parent:
            time_elem = parent.find('time')
            if time_elem:
                date = time_elem.get('datetime') or time_elem.get_text(strip=True)
        
        content_parts = []
        for sibling in heading.find_next_siblings(['p', 'ul', 'div'], limit=5):
            text = sibling.get_text(strip=True)
            if len(text) > 20 and ContentExtractor._is_valid_content(text):
                content_parts.append(text)
        
        content = ' '.join(content_parts)[:1000]
        
        if content or len(title) > 30:
            return Update(
                title=title[:250],
                content=content or title,
                source_type=SourceType.WEBSITE.value,
                url=url,
                date=date
            )
        return None
    
    @staticmethod
    def _extract_from_list_item(li, url: str) -> Optional[Update]:
        """Extract update from list item."""
        text = li.get_text(strip=True)
        if not ContentExtractor._is_valid_content(text) or len(text) < 15:
            return None
        
        # Try to find a heading within
        heading = li.find(['strong', 'b', 'h1', 'h2', 'h3', 'h4'])
        if heading:
            title = heading.get_text(strip=True)
            content = text.replace(title, '').strip()
        else:
            # Use first 100 chars as title, rest as content
            title = text[:100]
            content = text
        
        if len(title) > 250:
            title = title[:250]
        
        return Update(
            title=title,
            content=content[:1000],
            source_type=SourceType.WEBSITE.value,
            url=url
        )
    
    @staticmethod
    def _is_valid_content(text: str) -> bool:
        """Validate if text looks like actual content."""
        if not text or len(text) < 5 or len(text) > 500:
            return False
        
        # Reject UI garbage
        ui_keywords = ['filter', 'sort', 'toggle', 'menu', 'dropdown', 'select all', 
                      'show more', 'load more', 'click here', 'read more']
        if sum(1 for kw in ui_keywords if kw.lower() in text.lower()) >= 2:
            return False
        
        # Reject if mostly non-alphanumeric
        alphanumeric = sum(c.isalnum() or c.isspace() for c in text)
        if alphanumeric / len(text) < 0.7:
            return False
        
        # Reject camelCase spam
        caps_in_middle = sum(1 for i, c in enumerate(text) 
                            if i > 0 and c.isupper() and text[i-1].islower())
        if caps_in_middle > 4:
            return False
        
        return True

# --- AI Analysis ---
class AIAnalyzer:
    """AI-powered analysis of competitor updates."""
    
    @staticmethod
    def analyze_updates(competitor_name: str, category: str, 
                       updates: List[Update]) -> Optional[Analysis]:
        """Analyze updates using Gemini or fallback to rule-based."""
        if GEMINI_AVAILABLE and updates:
            result = AIAnalyzer._analyze_with_gemini(competitor_name, category, updates)
            if result:
                return result
        
        return AIAnalyzer._basic_analysis(competitor_name, category, updates)
    
    @staticmethod
    def _analyze_with_gemini(competitor_name: str, category: str, 
                            updates: List[Update]) -> Optional[Analysis]:
        """Use Gemini for sophisticated analysis."""
        try:
            model = genai.GenerativeModel('gemini-2.5-flash')
            
            # Build comprehensive but concise prompt
            prompt = f"""Analyze these competitor updates for strategic intelligence:

Competitor: {competitor_name}
Category: {category}
Number of Updates: {len(updates)}

Recent Updates:
"""
            for i, u in enumerate(updates[:5], 1):
                prompt += f"\n{i}. {u.title}"
                if u.date:
                    prompt += f" ({u.date})"
                prompt += "\n"
                if u.content and u.content != u.title:
                    preview = u.content[:350]
                    prompt += f"   {preview}{'...' if len(u.content) > 350 else ''}\n"
            
            prompt += """
Provide strategic analysis in JSON format:
{
  "summary": "One clear sentence summarizing what changed and why it matters",
  "key_features": ["specific feature 1", "specific feature 2", "specific feature 3"],
  "impact": "Critical/High/Medium/Low - business impact on market",
  "threat_level": "Critical/High/Medium/Low - competitive threat to us",
  "opportunities": ["opportunity 1", "opportunity 2"],
  "strategic_implications": "How this affects the competitive landscape",
  "action": "Specific recommended action for our team",
  "confidence_score": 0.0-1.0
}

Focus on:
- Business strategy implications
- Competitive positioning changes
- Market trends indicated
- Actionable insights"""

            response = model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.3,
                    max_output_tokens=600
                ),
                safety_settings={
                    'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE',
                    'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE',
                    'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE',
                    'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE',
                }
            )
            
            # Extract text from response
            text = AIAnalyzer._extract_response_text(response)
            text = re.sub(r'```json\s*|\s*```', '', text.strip())
            
            analysis_data = json.loads(text)
            
            return Analysis(
                summary=analysis_data.get('summary', 'Updated')[:400],
                key_features=analysis_data.get('key_features', [])[:5],
                impact=analysis_data.get('impact', 'Medium'),
                threat_level=analysis_data.get('threat_level', 'Medium'),
                opportunities=analysis_data.get('opportunities', [])[:4],
                strategic_implications=analysis_data.get('strategic_implications', '')[:400],
                action=analysis_data.get('action', 'Review changes')[:250],
                confidence_score=float(analysis_data.get('confidence_score', 0.7)),
                competitor_name=competitor_name
            )
            
        except json.JSONDecodeError as e:
            logger.warning(f"Failed to parse Gemini response for {competitor_name}: {e}")
            return None
        except Exception as e:
            logger.warning(f"Gemini analysis failed for {competitor_name}: {e}")
            return None
    
    @staticmethod
    def _extract_response_text(response) -> str:
        """Extract text from Gemini response safely."""
        if hasattr(response, 'text'):
            return response.text.strip()
        elif hasattr(response, 'candidates') and response.candidates:
            candidate = response.candidates[0]
            if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                return ''.join(part.text for part in candidate.content.parts 
                             if hasattr(part, 'text'))
        raise Exception("No text in response")
    
    @staticmethod
    def _basic_analysis(competitor_name: str, category: str, 
                       updates: List[Update]) -> Analysis:
        """Rule-based analysis fallback."""
        if not updates:
            return Analysis(
                summary=f"{competitor_name} has no recent updates",
                key_features=[],
                impact=ImpactLevel.LOW.value,
                threat_level=ImpactLevel.LOW.value,
                action="Continue monitoring for future updates",
                competitor_name=competitor_name,
                confidence_score=0.9
            )
        
        titles = [u.title for u in updates[:5]]
        contents = [u.content for u in updates[:5]]
        all_text = ' '.join(titles + contents).lower()
        
        # Advanced keyword-based impact assessment
        critical_keywords = ['acquisition', 'acquired', 'merger', 'funding', 'series', 
                           'million', 'billion', 'ipo', 'partnership with']
        high_keywords = ['launch', 'launched', 'new product', 'major update', 
                        'enterprise', 'pricing change', 'price', 'discount']
        medium_keywords = ['feature', 'improvement', 'update', 'release', 'beta',
                          'preview', 'integration', 'api']
        low_keywords = ['bug fix', 'patch', 'maintenance', 'minor']
        
        impact = ImpactLevel.LOW.value
        threat = ImpactLevel.LOW.value
        
        if any(kw in all_text for kw in critical_keywords):
            impact = ImpactLevel.CRITICAL.value
            threat = ImpactLevel.HIGH.value
        elif any(kw in all_text for kw in high_keywords):
            impact = ImpactLevel.HIGH.value
            threat = ImpactLevel.MEDIUM.value
        elif any(kw in all_text for kw in medium_keywords):
            impact = ImpactLevel.MEDIUM.value
            threat = ImpactLevel.LOW.value
        elif any(kw in all_text for kw in low_keywords):
            impact = ImpactLevel.LOW.value
            threat = ImpactLevel.LOW.value
        else:
            impact = ImpactLevel.MEDIUM.value
            threat = ImpactLevel.LOW.value
        
        # Generate opportunities
        opportunities = [
            "Monitor their approach and user reception",
            "Identify gaps in our own offering",
            "Consider if similar features would benefit our users"
        ]
        
        return Analysis(
            summary=f"{competitor_name} released: {titles[0]}" if titles else f"{competitor_name} updated",
            key_features=titles[:5],
            impact=impact,
            threat_level=threat,
            opportunities=opportunities,
            strategic_implications=f"{competitor_name} is actively evolving their {category} offering",
            action=f"Review {competitor_name}'s updates and assess competitive positioning",
            confidence_score=0.6,
            competitor_name=competitor_name
        )

# --- Trend Analysis ---
class TrendAnalyzer:
    """Analyze trends across competitors over time."""
    
    @staticmethod
    def record_finding(competitor_name: str, analysis: Analysis):
        """Record a finding for trend analysis."""
        trends = load_trends()
        
        if competitor_name not in trends:
            trends[competitor_name] = {
                'history': [],
                'impact_counts': defaultdict(int),
                'feature_mentions': defaultdict(int)
            }
        
        entry = {
            'timestamp': datetime.now().isoformat(),
            'impact': analysis.impact,
            'threat_level': analysis.threat_level,
            'summary': analysis.summary,
            'key_features': analysis.key_features
        }
        
        trends[competitor_name]['history'].append(entry)
        trends[competitor_name]['impact_counts'][analysis.impact] += 1
        
        # Track feature mentions
        for feature in analysis.key_features:
            # Extract key terms
            terms = set(word.lower() for word in feature.split() 
                       if len(word) > 4 and word.lower() not in ['feature', 'update', 'release'])
            for term in terms:
                trends[competitor_name]['feature_mentions'][term] += 1
        
        # Keep only last 50 entries
        if len(trends[competitor_name]['history']) > 50:
            trends[competitor_name]['history'] = trends[competitor_name]['history'][-50:]
        
        save_trends(trends)
    
    @staticmethod
    def get_competitor_trends(competitor_name: str) -> Dict[str, Any]:
        """Get trend analysis for a competitor."""
        trends = load_trends()
        
        if competitor_name not in trends:
            return {'error': 'No historical data'}
        
        data = trends[competitor_name]
        history = data['history']
        
        if not history:
            return {'error': 'No historical data'}
        
        # Calculate trends
        recent = history[-10:]  # Last 10 entries
        
        # Average impact level
        impact_scores = {
            'Critical': 4,
            'High': 3,
            'Medium': 2,
            'Low': 1
        }
        avg_impact = sum(impact_scores.get(entry['impact'], 2) for entry in recent) / len(recent)
        
        # Activity trend
        if len(history) >= 20:
            older = history[-20:-10]
            recent_activity = len(recent)
            older_activity = len(older)
            activity_trend = "increasing" if recent_activity > older_activity else "stable" if recent_activity == older_activity else "decreasing"
        else:
            activity_trend = "insufficient data"
        
        # Top mentioned features
        feature_counts = dict(data.get('feature_mentions', {}))
        top_features = sorted(feature_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        
        # Recent focus
        recent_keywords = []
        for entry in recent:
            for feature in entry.get('key_features', []):
                recent_keywords.extend(feature.lower().split())
        
        keyword_freq = defaultdict(int)
        for kw in recent_keywords:
            if len(kw) > 4 and kw not in ['feature', 'update', 'release', 'support']:
                keyword_freq[kw] += 1
        
        recent_focus = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        
        return {
            'total_updates': len(history),
            'recent_activity': len(recent),
            'activity_trend': activity_trend,
            'average_impact': avg_impact,
            'impact_distribution': dict(data['impact_counts']),
            'top_features': top_features,
            'recent_focus': recent_focus,
            'first_seen': history[0]['timestamp'] if history else None,
            'last_seen': history[-1]['timestamp'] if history else None
        }

# --- Monitoring Engine ---
class MonitoringEngine:
    """Core monitoring and analysis engine."""
    
    def __init__(self):
        self.config = load_config()
        self.cache = load_cache()
        self.notifier = EmailNotifier(self.config)
    
    def monitor_competitor(self, competitor: CompetitorConfig) -> Optional[Dict[str, Any]]:
        """Monitor a single competitor for updates."""
        if not competitor.enabled:
            logger.info(f"Skipping disabled competitor: {competitor.name}")
            return None
        
        logger.info(f"üîç Monitoring {competitor.name}...")
        
        updates = []
        
        # Try RSS first (most reliable)
        if competitor.rss_feed_url:
            try:
                rss_updates = ContentExtractor.extract_from_rss(competitor.rss_feed_url)
                if rss_updates:
                    updates.extend(rss_updates)
                    logger.info(f"  ‚úì Found {len(rss_updates)} RSS updates")
            except Exception as e:
                logger.warning(f"  RSS failed: {e}")
        
        # Try website if no RSS or need more
        if (not updates or len(updates) < 3) and competitor.website_url:
            try:
                web_updates = ContentExtractor.extract_from_website(competitor.website_url)
                if web_updates:
                    # Avoid duplicates
                    existing_titles = {u.title for u in updates}
                    new_web_updates = [u for u in web_updates if u.title not in existing_titles]
                    updates.extend(new_web_updates)
                    logger.info(f"  ‚úì Found {len(new_web_updates)} website updates")
            except Exception as e:
                logger.warning(f"  Website failed: {e}")
        
        if not updates:
            logger.info(f"  ‚ÑπÔ∏è  No updates found for {competitor.name}")
            return None
        
        # Filter for new content
        new_updates = self._filter_new_updates(competitor.name, updates)
        
        if not new_updates:
            logger.info(f"  ‚ÑπÔ∏è  All content previously seen for {competitor.name}")
            # Update last checked time
            competitor.last_checked = datetime.now().isoformat()
            self._update_competitor_config(competitor)
            return None
        
        logger.info(f"  üÜï {len(new_updates)} new updates for {competitor.name}")
        
        # Analyze updates
        analysis = AIAnalyzer.analyze_updates(
            competitor.name, 
            competitor.category, 
            new_updates
        )
        
        if not analysis:
            logger.warning(f"  ‚ö†Ô∏è  Analysis failed for {competitor.name}")
            return None
        
        logger.info(f"  üìä Impact: {analysis.impact} | Threat: {analysis.threat_level}")
        
        # Record for trend analysis
        TrendAnalyzer.record_finding(competitor.name, analysis)
        
        # Send alert if high impact
        if analysis.impact in ['Critical', 'High']:
            self.notifier.send_alert(competitor.name, analysis.impact, analysis.summary)
        
        # Update cache and competitor config
        self._update_cache(competitor.name, updates)
        competitor.last_checked = datetime.now().isoformat()
        self._update_competitor_config(competitor)
        
        return {
            'competitor': competitor,
            'updates': new_updates[:self.config.max_updates_per_competitor],
            'analysis': analysis
        }
    
    def _filter_new_updates(self, competitor_name: str, 
                           updates: List[Update]) -> List[Update]:
        """Filter out previously seen updates."""
        cache_key = f"competitor_{competitor_name}"
        seen_hashes = set(self.cache.get(cache_key, []))
        
        new_updates = []
        for update in updates:
            update_hash = hashlib.md5(
                f"{update.title}{update.content[:100]}".encode()
            ).hexdigest()
            
            if update_hash not in seen_hashes:
                new_updates.append(update)
        
        return new_updates
    
    def _update_cache(self, competitor_name: str, updates: List[Update]):
        """Update cache with seen updates."""
        cache_key = f"competitor_{competitor_name}"
        
        update_hashes = [
            hashlib.md5(f"{u.title}{u.content[:100]}".encode()).hexdigest()
            for u in updates
        ]
        
        # Keep last 100 hashes per competitor
        self.cache[cache_key] = update_hashes[:100]
        save_cache(self.cache)
    
    def _update_competitor_config(self, competitor: CompetitorConfig):
        """Update a single competitor's configuration."""
        competitors = load_competitors()
        for i, comp in enumerate(competitors):
            if comp.name == competitor.name:
                competitors[i] = competitor
                break
        save_competitors(competitors)
    
    def monitor_all(self) -> List[Dict[str, Any]]:
        """Monitor all enabled competitors."""
        competitors = load_competitors()
        findings = []
        
        enabled_competitors = [c for c in competitors if c.enabled]
        logger.info(f"Monitoring {len(enabled_competitors)} enabled competitors...")
        
        for i, competitor in enumerate(enabled_competitors, 1):
            try:
                logger.info(f"[{i}/{len(enabled_competitors)}] Checking {competitor.name}")
                result = self.monitor_competitor(competitor)
                if result:
                    findings.append(result)
                
                # Rate limiting
                if i < len(enabled_competitors):
                    time.sleep(self.config.rate_limit_seconds)
                    
            except Exception as e:
                logger.error(f"Error monitoring {competitor.name}: {e}", exc_info=True)
        
        return findings

# --- Export Functions ---
class ExportManager:
    """Export intelligence reports in various formats."""
    
    @staticmethod
    def export_to_json(findings: List[Dict[str, Any]], filename: Optional[str] = None) -> Path:
        """Export findings to JSON."""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"intel_export_{timestamp}.json"
        
        export_path = EXPORTS_DIR / filename
        
        # Convert dataclasses to dicts
        export_data = []
        for finding in findings:
            export_data.append({
                'competitor': asdict(finding['competitor']),
                'updates': [asdict(u) for u in finding['updates']],
                'analysis': asdict(finding['analysis'])
            })
        
        with open(export_path, 'w') as f:
            json.dump({
                'generated_at': datetime.now().isoformat(),
                'findings_count': len(findings),
                'findings': export_data
            }, f, indent=2)
        
        logger.info(f"Exported to JSON: {export_path}")
        return export_path
    
    @staticmethod
    def export_to_csv(findings: List[Dict[str, Any]], filename: Optional[str] = None) -> Path:
        """Export findings to CSV."""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"intel_export_{timestamp}.csv"
        
        export_path = EXPORTS_DIR / filename
        
        with open(export_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'Competitor', 'Category', 'Priority', 'Impact', 'Threat Level',
                'Summary', 'Key Features', 'Action', 'Update Count', 'Detected At'
            ])
            
            for finding in findings:
                comp = finding['competitor']
                analysis = finding['analysis']
                
                writer.writerow([
                    comp.name,
                    comp.category,
                    comp.priority,
                    analysis.impact,
                    analysis.threat_level,
                    analysis.summary,
                    '; '.join(analysis.key_features),
                    analysis.action,
                    len(finding['updates']),
                    datetime.now().isoformat()
                ])
        
        logger.info(f"Exported to CSV: {export_path}")
        return export_path

# --- Report Generation ---
class ReportGenerator:
    """Generate various formats of intelligence reports."""
    
    @staticmethod
    def generate_digest(findings: List[Dict[str, Any]], 
                       format_type: str = "detailed") -> str:
        """Generate intelligence digest."""
        if not findings:
            return "‚úÖ No competitor updates detected in this check.\n\n" + \
                   "All monitored competitors have been scanned. No new activity found."
        
        if format_type == "brief":
            return ReportGenerator._generate_brief_digest(findings)
        elif format_type == "summary":
            return ReportGenerator._generate_summary_digest(findings)
        else:
            return ReportGenerator._generate_detailed_digest(findings)
    
    @staticmethod
    def _generate_brief_digest(findings: List[Dict[str, Any]]) -> str:
        """Generate brief digest - just headlines."""
        report = f"# üìä Competitive Intelligence Brief\n\n"
        report += f"**{datetime.now().strftime('%B %d, %Y')}**\n\n"
        report += f"**{len(findings)} competitors with updates:**\n\n"
        
        # Sort by impact
        findings_sorted = sorted(
            findings,
            key=lambda x: ['Critical', 'High', 'Medium', 'Low'].index(x['analysis'].impact)
        )
        
        for f in findings_sorted:
            competitor = f['competitor']
            analysis = f['analysis']
            impact_emoji = ReportGenerator._get_impact_emoji(analysis.impact)
            
            report += f"{impact_emoji} **{competitor.name}** ({analysis.impact}): {analysis.summary}\n"
        
        return report
    
    @staticmethod
    def _generate_summary_digest(findings: List[Dict[str, Any]]) -> str:
        """Generate summary digest - key info only."""
        report = f"# üìä Competitive Intelligence Summary\n\n"
        report += f"**{datetime.now().strftime('%B %d, %Y at %I:%M %p')}**\n\n"
        
        # Statistics
        total_updates = sum(len(f['updates']) for f in findings)
        report += f"**Updates Found:** {total_updates} from {len(findings)} competitors\n\n"
        
        # Group by impact
        by_impact = defaultdict(list)
        for f in findings:
            by_impact[f['analysis'].impact].append(f)
        
        # Show high-impact first
        for impact in [ImpactLevel.CRITICAL.value, ImpactLevel.HIGH.value, 
                      ImpactLevel.MEDIUM.value, ImpactLevel.LOW.value]:
            if impact in by_impact:
                emoji = ReportGenerator._get_impact_emoji(impact)
                report += f"## {emoji} {impact} Impact ({len(by_impact[impact])})\n\n"
                
                for f in by_impact[impact]:
                    competitor = f['competitor']
                    analysis = f['analysis']
                    
                    report += f"### {competitor.name}\n\n"
                    report += f"**Category:** {competitor.category} | **Priority:** {competitor.priority}\n\n"
                    report += f"**Summary:** {analysis.summary}\n\n"
                    
                    if analysis.key_features:
                        report += "**Key Changes:**\n"
                        for feat in analysis.key_features[:3]:
                            report += f"- {feat}\n"
                        report += "\n"
                    
                    report += f"üí° **Action:** {analysis.action}\n\n"
                    report += "---\n\n"
        
        return report
    
    @staticmethod
    def _generate_detailed_digest(findings: List[Dict[str, Any]]) -> str:
        """Generate detailed digest - full intelligence report."""
        report = f"# üéØ Competitive Intelligence Report\n\n"
        report += f"**Generated:** {datetime.now().strftime('%B %d, %Y at %I:%M %p')}\n\n"
        
        # Executive Summary
        report += "## üìã Executive Summary\n\n"
        
        total_updates = sum(len(f['updates']) for f in findings)
        report += f"This report covers **{total_updates} updates** from **{len(findings)} competitors**.\n\n"
        
        # Impact breakdown
        impact_counts = defaultdict(int)
        for f in findings:
            impact_counts[f['analysis'].impact] += 1
        
        report += "**Impact Breakdown:**\n"
        for impact in [ImpactLevel.CRITICAL.value, ImpactLevel.HIGH.value, 
                      ImpactLevel.MEDIUM.value, ImpactLevel.LOW.value]:
            if impact in impact_counts:
                emoji = ReportGenerator._get_impact_emoji(impact)
                report += f"- {emoji} {impact}: {impact_counts[impact]} competitor(s)\n"
        report += "\n"
        
        # Key highlights
        report += "**Key Highlights:**\n"
        for f in sorted(findings, 
                       key=lambda x: ['Critical', 'High', 'Medium', 'Low'].index(x['analysis'].impact))[:5]:
            competitor = f['competitor']
            analysis = f['analysis']
            report += f"- **{competitor.name}**: {analysis.summary}\n"
        report += "\n"
        
        report += "---\n\n"
        
        # Detailed Analysis
        report += "## üîç Detailed Analysis\n\n"
        
        # Sort by impact level
        findings_sorted = sorted(
            findings, 
            key=lambda x: (
                ['Critical', 'High', 'Medium', 'Low'].index(x['analysis'].impact),
                x['competitor'].name
            )
        )
        
        for f in findings_sorted:
            competitor = f['competitor']
            analysis = f['analysis']
            updates = f['updates']
            
            report += f"### {competitor.name}\n\n"
            
            # Metadata
            tags_str = ", ".join(competitor.tags) if competitor.tags else "None"
            report += f"**Category:** {competitor.category} | "
            report += f"**Priority:** {competitor.priority} | "
            report += f"**Tags:** {tags_str}\n\n"
            
            # Impact indicators
            impact_emoji = ReportGenerator._get_impact_emoji(analysis.impact)
            threat_emoji = ReportGenerator._get_impact_emoji(analysis.threat_level)
            
            report += f"{impact_emoji} **Impact:** {analysis.impact} | "
            report += f"{threat_emoji} **Threat Level:** {analysis.threat_level}\n\n"
            
            # Analysis
            report += f"**Summary:** {analysis.summary}\n\n"
            
            if analysis.key_features:
                report += "**Key Features/Changes:**\n"
                for feat in analysis.key_features:
                    report += f"- {feat}\n"
                report += "\n"
            
            if analysis.strategic_implications:
                report += f"**Strategic Implications:** {analysis.strategic_implications}\n\n"
            
            if analysis.opportunities:
                report += "**Opportunities for Us:**\n"
                for opp in analysis.opportunities:
                    report += f"- {opp}\n"
                report += "\n"
            
            report += f"üí° **Recommended Action:** {analysis.action}\n\n"
            
            if hasattr(analysis, 'confidence_score') and analysis.confidence_score:
                confidence_pct = int(analysis.confidence_score * 100)
                report += f"*Analysis Confidence: {confidence_pct}%*\n\n"
            
            # Show updates in collapsible section
            report += "<details>\n<summary>üìÑ View All Updates ({len(updates)})</summary>\n\n"
            for i, u in enumerate(updates, 1):
                report += f"#### Update {i}: {u.title}\n\n"
                if u.date:
                    report += f"*{u.date}* | "
                report += f"*Source: {u.source_type.upper()}*\n\n"
                
                if u.content and u.content != u.title:
                    content_preview = u.content[:600]
                    if len(u.content) > 600:
                        content_preview += "..."
                    report += f"{content_preview}\n\n"
                
                report += f"[View Source ‚Üí]({u.url})\n\n"
            report += "</details>\n\n"
            
            # Add link to main source
            source_url = competitor.rss_feed_url or competitor.website_url
            if source_url:
                report += f"üîó [View All {competitor.name} Updates]({source_url})\n\n"
            
            report += "---\n\n"
        
        # Add metadata footer
        report += "## ‚ÑπÔ∏è Report Information\n\n"
        report += f"- **Generated:** {datetime.now().isoformat()}\n"
        report += f"- **Competitors Monitored:** {len(findings)}\n"
        report += f"- **Total Updates:** {total_updates}\n"
        report += f"- **Analysis Engine:** {'Gemini 2.5 Flash (AI-powered)' if GEMINI_AVAILABLE else 'Rule-based analysis'}\n"
        report += f"- **Report Format:** Detailed\n\n"
        
        report += "*Generated by IntelTracker v12.0*\n"
        
        return report
    
    @staticmethod
    def _get_impact_emoji(impact: str) -> str:
        """Get emoji for impact level."""
        emoji_map = {
            ImpactLevel.CRITICAL.value: "üî¥",
            ImpactLevel.HIGH.value: "üü†",
            ImpactLevel.MEDIUM.value: "üü°",
            ImpactLevel.LOW.value: "üü¢"
        }
        return emoji_map.get(impact, "‚ö™")
    
    @staticmethod
    def save_report(report: str, filename: Optional[str] = None) -> Path:
        """Save report to file."""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"intel_report_{timestamp}.md"
        
        report_path = REPORTS_DIR / filename
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"Report saved: {report_path}")
        return report_path

# --- ADK Tools ---
@FunctionTool
def run_intelligence_check(
    tool_context: "ToolContext",
    format: str = "detailed",
    send_email: bool = False
) -> str:
    """
    Run competitive intelligence check across all enabled competitors.
    
    Args:
        format: Report format - 'brief', 'summary', or 'detailed' (default)
        send_email: Send report via email if configured (default: False)
    
    Returns:
        Intelligence report with competitor updates and analysis
    """
    logger.info("=" * 70)
    logger.info("Starting intelligence check...")
    logger.info("=" * 70)
    
    config = load_config()
    engine = MonitoringEngine()
    findings = engine.monitor_all()
    
    if not findings:
        message = "‚úÖ **No New Updates Detected**\n\n" + \
                 "All enabled competitors have been scanned. No new activity found since last check.\n\n" + \
                 "This is normal - it means your competitors haven't made significant public changes recently."
        
        tool_context.state['last_check'] = datetime.now().isoformat()
        tool_context.state['last_findings_count'] = 0
        
        return message
    
    # Generate report
    report = ReportGenerator.generate_digest(findings, format)
    
    # Save report
    report_path = ReportGenerator.save_report(report)
    
    # Export if configured
    if config.export_after_check:
        ExportManager.export_to_json(findings)
        ExportManager.export_to_csv(findings)
    
    # Send email if requested
    email_status = ""
    if send_email or (config.notification_email and findings):
        notifier = EmailNotifier(config)
        if notifier.send_digest(report, len(findings)):
            email_status = f"\n\nüìß Report emailed to {config.notification_email}"
        else:
            email_status = "\n\n‚ö†Ô∏è Email sending failed (check SMTP configuration)"
    
    # Update state
    tool_context.state['last_check'] = datetime.now().isoformat()
    tool_context.state['last_findings_count'] = len(findings)
    
    summary = f"üéØ **{len(findings)} Competitor Update(s) Found**\n\n"
    summary += f"Report saved: `{report_path.name}`{email_status}\n\n"
    summary += "---\n\n"
    summary += report
    
    return summary

@FunctionTool
def add_competitor(
    name: str,
    tool_context: "ToolContext",
    website_url: Optional[str] = None,
    rss_feed_url: Optional[str] = None,
    twitter_handle: Optional[str] = None,
    linkedin_url: Optional[str] = None,
    press_release_url: Optional[str] = None,
    category: Optional[str] = None,
    priority: str = "Medium",
    tags: Optional[str] = None
) -> str:
    """
    Add a new competitor to track.
    
    Args:
        name: Competitor name (required)
        website_url: Main website or changelog URL
        rss_feed_url: RSS feed URL if available (most reliable)
        twitter_handle: Twitter handle (without @)
        linkedin_url: LinkedIn company page URL
        press_release_url: Press release RSS/page URL
        category: Business category (e.g., "Productivity", "CRM")
        priority: Priority level - "High", "Medium", or "Low" (default: "Medium")
        tags: Comma-separated tags (e.g., "saas,enterprise,ai")
    
    Returns:
        Confirmation message
    """
    if not website_url and not rss_feed_url:
        return "‚ùå Please provide at least a website_url or rss_feed_url to monitor."
    
    competitors = load_competitors()
    
    # Check if already exists
    if any(c.name.lower() == name.lower() for c in competitors):
        return f"‚ùå Competitor '{name}' is already being tracked.\n\n" + \
               "Use `toggle_competitor` to enable it if disabled, or `remove_competitor` to delete and re-add."
    
    # Parse tags
    tag_list = []
    if tags:
        tag_list = [t.strip() for t in tags.split(',') if t.strip()]
    
    # Create new competitor
    new_competitor = CompetitorConfig(
        name=name,
        website_url=website_url,
        rss_feed_url=rss_feed_url,
        twitter_handle=twitter_handle,
        linkedin_url=linkedin_url,
        press_release_url=press_release_url,
        category=category or "Unknown",
        priority=priority,
        tags=tag_list
    )
    
    competitors.append(new_competitor)
    save_competitors(competitors)
    
    logger.info(f"Added competitor: {name}")
    
    sources = []
    if website_url:
        sources.append("Website")
    if rss_feed_url:
        sources.append("RSS (recommended)")
    if twitter_handle:
        sources.append("Twitter")
    if linkedin_url:
        sources.append("LinkedIn")
    if press_release_url:
        sources.append("Press Releases")
    
    return f"‚úÖ **Successfully added '{name}' to monitoring list**\n\n" + \
           f"**Category:** {new_competitor.category}\n" + \
           f"**Priority:** {priority}\n" + \
           f"**Sources:** {', '.join(sources)}\n" + \
           f"**Tags:** {', '.join(tag_list) if tag_list else 'None'}\n\n" + \
           f"Run `run_intelligence_check` to start monitoring immediately, or it will be included in the next scheduled check."

@FunctionTool
def list_competitors(tool_context: "ToolContext", show_disabled: bool = True) -> str:
    """
    Show all tracked competitors with their status and configuration.
    
    Args:
        show_disabled: Include disabled competitors in the list (default: True)
    
    Returns:
        Formatted list of competitors
    """
    competitors = load_competitors()
    
    if not competitors:
        return "üì≠ **No competitors are currently being tracked.**\n\n" + \
               "Use `add_competitor` to start monitoring competitors.\n\n" + \
               "**Quick start example:**\n" + \
               "```\n" + \
               "add_competitor(\n" + \
               "  name='CompetitorName',\n" + \
               "  website_url='https://competitor.com/blog',\n" + \
               "  category='SaaS'\n" + \
               ")\n" + \
               "```"
    
    # Filter if needed
    if not show_disabled:
        competitors = [c for c in competitors if c.enabled]
    
    # Group by priority
    by_priority = {'High': [], 'Medium': [], 'Low': []}
    for comp in competitors:
        by_priority[comp.priority].append(comp)
    
    enabled_count = sum(1 for c in competitors if c.enabled)
    disabled_count = len(competitors) - enabled_count
    
    result = f"# üìä Tracked Competitors\n\n"
    result += f"**Total:** {len(competitors)} | **Active:** {enabled_count} | **Disabled:** {disabled_count}\n\n"
    
    for priority in ['High', 'Medium', 'Low']:
        if by_priority[priority]:
            emoji = {"High": "üî¥", "Medium": "üü°", "Low": "üü¢"}[priority]
            result += f"## {emoji} {priority} Priority ({len(by_priority[priority])})\n\n"
            
            for comp in by_priority[priority]:
                status_emoji = "‚úÖ" if comp.enabled else "‚ùå"
                result += f"### {status_emoji} {comp.name}\n\n"
                result += f"- **Status:** {'Active' if comp.enabled else 'Disabled'}\n"
                result += f"- **Category:** {comp.category}\n"
                
                # Sources
                sources = []
                if comp.website_url:
                    sources.append(f"[Website]({comp.website_url})")
                if comp.rss_feed_url:
                    sources.append("RSS ‚≠ê")
                if comp.twitter_handle:
                    sources.append(f"Twitter (@{comp.twitter_handle})")
                if comp.linkedin_url:
                    sources.append("LinkedIn")
                if comp.press_release_url:
                    sources.append("Press")
                
                result += f"- **Sources:** {', '.join(sources) if sources else 'None configured'}\n"
                
                if comp.tags:
                    result += f"- **Tags:** {', '.join(comp.tags)}\n"
                
                result += f"- **Added:** {comp.added_at[:10]}\n"
                
                if comp.last_checked:
                    last_check = datetime.fromisoformat(comp.last_checked)
                    result += f"- **Last Checked:** {last_check.strftime('%b %d at %I:%M %p')}\n"
                
                result += "\n"
    
    result += "---\n\n"
    result += "üí° **Tips:**\n"
    result += "- Use `check_single_competitor('name')` to check a specific competitor\n"
    result += "- Use `toggle_competitor('name')` to enable/disable monitoring\n"
    result += "- Use `view_competitor_trends('name')` to see historical patterns\n"
    
    return result

@FunctionTool
def remove_competitor(name: str, tool_context: "ToolContext", confirm: bool = False) -> str:
    """
    Remove a competitor from tracking.
    
    Args:
        name: Name of competitor to remove
        confirm: Set to True to confirm deletion (safety check)
    
    Returns:
        Confirmation message
    """
    if not confirm:
        return f"‚ö†Ô∏è **Confirm Deletion**\n\n" + \
               f"Are you sure you want to remove '{name}' from tracking?\n\n" + \
               "This will delete all monitoring configuration. Historical data will be preserved.\n\n" + \
               f"To confirm, use: `remove_competitor('{name}', confirm=True)`"
    
    competitors = load_competitors()
    original_count = len(competitors)
    
    removed_competitor = None
    competitors_filtered = []
    for c in competitors:
        if c.name.lower() == name.lower():
            removed_competitor = c
        else:
            competitors_filtered.append(c)
    
    if len(competitors_filtered) == original_count:
        return f"‚ùå Competitor '{name}' not found in tracking list.\n\n" + \
               "Use `list_competitors` to see all tracked competitors."
    
    save_competitors(competitors_filtered)
    logger.info(f"Removed competitor: {name}")
    
    return f"‚úÖ **Removed '{name}' from monitoring list**\n\n" + \
           f"**Category:** {removed_competitor.category}\n" + \
           f"**Priority:** {removed_competitor.priority}\n\n" + \
           "Historical trend data has been preserved. You can re-add this competitor anytime."

@FunctionTool
def toggle_competitor(name: str, tool_context: "ToolContext") -> str:
    """
    Enable or disable monitoring for a competitor without removing them.
    
    Args:
        name: Name of competitor to toggle
    
    Returns:
        Confirmation message with new status
    """
    competitors = load_competitors()
    
    found = False
    new_status = None
    for comp in competitors:
        if comp.name.lower() == name.lower():
            comp.enabled = not comp.enabled
            new_status = comp.enabled
            found = True
            break
    
    if not found:
        return f"‚ùå Competitor '{name}' not found.\n\n" + \
               "Use `list_competitors` to see all tracked competitors."
    
    save_competitors(competitors)
    logger.info(f"Toggled competitor {name}: {'enabled' if new_status else 'disabled'}")
    
    status_text = "**enabled** ‚úÖ" if new_status else "**disabled** ‚ùå"
    
    message = f"‚úÖ Monitoring for '{name}' is now {status_text}\n\n"
    
    if new_status:
        message += "This competitor will be included in the next intelligence check."
    else:
        message += "This competitor will be skipped in intelligence checks until re-enabled.\n" + \
                  "All configuration and historical data are preserved."
    
    return message

@FunctionTool
def check_single_competitor(name: str, tool_context: "ToolContext") -> str:
    """
    Check a specific competitor for updates immediately.
    
    Args:
        name: Name of competitor to check
    
    Returns:
        Updates and analysis for this competitor
    """
    competitors = load_competitors()
    
    competitor = None
    for comp in competitors:
        if comp.name.lower() == name.lower():
            competitor = comp
            break
    
    if not competitor:
        return f"‚ùå Competitor '{name}' not found.\n\n" + \
               "Use `list_competitors` to see all tracked competitors, or `add_competitor` to add a new one."
    
    if not competitor.enabled:
        return f"‚ö†Ô∏è '{name}' is currently disabled.\n\n" + \
               f"Use `toggle_competitor('{name}')` to enable monitoring, then try again."
    
    logger.info(f"Checking single competitor: {name}")
    
    engine = MonitoringEngine()
    result = engine.monitor_competitor(competitor)
    
    if not result:
        return f"‚úÖ **No new updates found for {name}**\n\n" + \
               f"**Last Checked:** {datetime.now().strftime('%B %d at %I:%M %p')}\n\n" + \
               "This competitor is being monitored, but hasn't published new content since the last check."
    
    # Generate focused report
    analysis = result['analysis']
    updates = result['updates']
    
    impact_emoji = ReportGenerator._get_impact_emoji(analysis.impact)
    threat_emoji = ReportGenerator._get_impact_emoji(analysis.threat_level)
    
    report = f"# üìä {name} Intelligence Update\n\n"
    report += f"**Checked:** {datetime.now().strftime('%B %d, %Y at %I:%M %p')}\n"
    report += f"**New Updates:** {len(updates)}\n\n"
    
    report += f"{impact_emoji} **Impact:** {analysis.impact} | "
    report += f"{threat_emoji} **Threat:** {analysis.threat_level}\n\n"
    
    report += "## Summary\n\n"
    report += f"{analysis.summary}\n\n"
    
    if analysis.key_features:
        report += "## Key Changes\n\n"
        for feat in analysis.key_features:
            report += f"- {feat}\n"
        report += "\n"
    
    if analysis.strategic_implications:
        report += f"**Strategic Implications:** {analysis.strategic_implications}\n\n"
    
    if analysis.opportunities:
        report += "## Opportunities\n\n"
        for opp in analysis.opportunities:
            report += f"- {opp}\n"
        report += "\n"
    
    report += f"## Recommended Action\n\n{analysis.action}\n\n"
    
    report += "## Detailed Updates\n\n"
    for i, u in enumerate(updates, 1):
        report += f"### {i}. {u.title}\n\n"
        if u.date:
            report += f"*{u.date}* | "
        report += f"*Source: {u.source_type.upper()}*\n\n"
        
        if u.content and u.content != u.title:
            report += f"{u.content}\n\n"
        
        report += f"[View Source ‚Üí]({u.url})\n\n"
    
    return report

@FunctionTool
def view_competitor_trends(name: str, tool_context: "ToolContext") -> str:
    """
    View historical trends and patterns for a competitor.
    
    Args:
        name: Name of competitor
    
    Returns:
        Trend analysis and historical patterns
    """
    trends = TrendAnalyzer.get_competitor_trends(name)
    
    if 'error' in trends:
        return f"üìä **No historical data for {name}**\n\n" + \
               "This competitor hasn't been checked yet, or no updates have been found.\n\n" + \
               f"Run `check_single_competitor('{name}')` to start collecting data."
    
    report = f"# üìà Trend Analysis: {name}\n\n"
    
    # Overview
    report += "## Overview\n\n"
    report += f"- **Total Updates Tracked:** {trends['total_updates']}\n"
    report += f"- **Recent Activity:** {trends['recent_activity']} updates in last 10 checks\n"
    report += f"- **Activity Trend:** {trends['activity_trend'].title()}\n"
    report += f"- **Average Impact:** {trends['average_impact']:.1f}/4.0\n\n"
    
    if trends['first_seen']:
        first = datetime.fromisoformat(trends['first_seen'])
        last = datetime.fromisoformat(trends['last_seen'])
        report += f"- **First Tracked:** {first.strftime('%B %d, %Y')}\n"
        report += f"- **Last Update:** {last.strftime('%B %d, %Y')}\n\n"
    
    # Impact distribution
    report += "## Impact Distribution\n\n"
    if trends['impact_distribution']:
        for impact in ['Critical', 'High', 'Medium', 'Low']:
            count = trends['impact_distribution'].get(impact, 0)
            if count > 0:
                emoji = ReportGenerator._get_impact_emoji(impact)
                report += f"{emoji} **{impact}:** {count}\n"
        report += "\n"
    
    # Top features
    if trends['top_features']:
        report += "## Most Mentioned Features\n\n"
        for feature, count in trends['top_features'][:10]:
            report += f"- **{feature}:** {count} mentions\n"
        report += "\n"
    
    # Recent focus
    if trends['recent_focus']:
        report += "## Recent Focus Areas\n\n"
        report += "Keywords from last 10 updates:\n"
        for keyword, count in trends['recent_focus']:
            report += f"- {keyword} ({count}x)\n"
        report += "\n"
    
    report += "---\n\n"
    report += "üí° This trend data helps identify:\n"
    report += "- How active this competitor is\n"
    report += "- What areas they're focusing on\n"
    report += "- Whether their impact on the market is increasing\n"
    
    return report

@FunctionTool
def configure_monitoring(
    tool_context: "ToolContext",
    check_interval_hours: Optional[int] = None,
    enable_scheduling: Optional[bool] = None,
    notification_email: Optional[str] = None,
    smtp_username: Optional[str] = None,
    smtp_password: Optional[str] = None,
    digest_format: Optional[str] = None,
    notify_on_critical: Optional[bool] = None,
    notify_on_high: Optional[bool] = None,
    export_after_check: Optional[bool] = None,
    max_updates_per_competitor: Optional[int] = None
) -> str:
    """
    Configure monitoring settings and preferences.
    
    Args:
        check_interval_hours: How often to check (hours) for scheduled monitoring
        enable_scheduling: Enable automatic scheduled checks
        notification_email: Email address for notifications
        smtp_username: SMTP username (usually your email)
        smtp_password: SMTP password or app-specific password
        digest_format: Report format - 'brief', 'summary', or 'detailed'
        notify_on_critical: Send immediate alerts for critical updates
        notify_on_high: Send immediate alerts for high impact updates
        export_after_check: Auto-export to JSON/CSV after each check
        max_updates_per_competitor: Maximum updates to include per competitor
    
    Returns:
        Configuration confirmation with current settings
    """
    config = load_config()
    changes = []
    
    if check_interval_hours is not None:
        if check_interval_hours < 1:
            return "‚ùå Check interval must be at least 1 hour."
        config.check_interval_hours = check_interval_hours
        changes.append(f"Check interval: {check_interval_hours} hours")
    
    if enable_scheduling is not None:
        config.enable_scheduling = enable_scheduling
        changes.append(f"Scheduling: {'enabled' if enable_scheduling else 'disabled'}")
    
    if notification_email is not None:
        config.notification_email = notification_email
        changes.append(f"Notification email: {notification_email}")
    
    if smtp_username is not None:
        config.smtp_username = smtp_username
        changes.append(f"SMTP username: {smtp_username}")
    
    if smtp_password is not None:
        config.smtp_password = smtp_password
        changes.append("SMTP password: [updated]")
    
    if digest_format is not None:
        if digest_format not in ['brief', 'summary', 'detailed']:
            return "‚ùå Invalid format. Use: 'brief', 'summary', or 'detailed'"
        config.digest_format = digest_format
        changes.append(f"Report format: {digest_format}")
    
    if notify_on_critical is not None:
        config.notify_on_critical = notify_on_critical
        changes.append(f"Critical alerts: {'enabled' if notify_on_critical else 'disabled'}")
    
    if notify_on_high is not None:
        config.notify_on_high = notify_on_high
        changes.append(f"High impact alerts: {'enabled' if notify_on_high else 'disabled'}")
    
    if export_after_check is not None:
        config.export_after_check = export_after_check
        changes.append(f"Auto-export: {'enabled' if export_after_check else 'disabled'}")
    
    if max_updates_per_competitor is not None:
        if max_updates_per_competitor < 1:
            return "‚ùå Must allow at least 1 update per competitor."
        config.max_updates_per_competitor = max_updates_per_competitor
        changes.append(f"Max updates/competitor: {max_updates_per_competitor}")
    
    save_config(config)
    
    result = "‚öôÔ∏è **Monitoring Configuration**\n\n"
    
    if changes:
        result += "**Updated Settings:**\n"
        for change in changes:
            result += f"- {change}\n"
        result += "\n---\n\n"
    
    result += "**Current Configuration:**\n\n"
    result += f"- **Check Interval:** {config.check_interval_hours} hours\n"
    result += f"- **Scheduled Monitoring:** {'‚úÖ Enabled' if config.enable_scheduling else '‚ùå Disabled'}\n"
    result += f"- **Report Format:** {config.digest_format.title()}\n"
    result += f"- **Max Updates/Competitor:** {config.max_updates_per_competitor}\n"
    result += f"- **Auto-Export:** {'‚úÖ Yes' if config.export_after_check else '‚ùå No'}\n\n"
    
    result += "**Email Notifications:**\n"
    if config.notification_email:
        result += f"- **Email:** {config.notification_email}\n"
        result += f"- **SMTP Server:** {config.smtp_server}:{config.smtp_port}\n"
        result += f"- **Critical Alerts:** {'‚úÖ' if config.notify_on_critical else '‚ùå'}\n"
        result += f"- **High Impact Alerts:** {'‚úÖ' if config.notify_on_high else '‚ùå'}\n"
    else:
        result += "- Not configured (set notification_email to enable)\n"
    
    result += "\nüí° **Email Setup Tip:** For Gmail, use an [App Password](https://myaccount.google.com/apppasswords)"
    
    return result

@FunctionTool
def export_intelligence_data(
    tool_context: "ToolContext",
    format: str = "json",
    include_trends: bool = False
) -> str:
    """
    Export intelligence data for external analysis or backup.
    
    Args:
        format: Export format - 'json' or 'csv' (default: 'json')
        include_trends: Include historical trend data (JSON only)
    
    Returns:
        Export confirmation with file path
    """
    # Get latest findings from state or run new check
    logger.info("Preparing data export...")
    
    # For now, we need to run a fresh check
    engine = MonitoringEngine()
    findings = engine.monitor_all()
    
    if not findings:
        return "‚ÑπÔ∏è No data to export. Run `run_intelligence_check` first to gather intelligence data."
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    if format.lower() == "csv":
        export_path = ExportManager.export_to_csv(findings, f"intel_export_{timestamp}.csv")
        
        return f"‚úÖ **Data exported to CSV**\n\n" + \
               f"**File:** `{export_path.name}`\n" + \
               f"**Location:** `{export_path}`\n" + \
               f"**Records:** {len(findings)} competitors\n\n" + \
               "CSV format is suitable for spreadsheet analysis and data visualization tools."
    
    else:  # JSON
        export_path = ExportManager.export_to_json(findings, f"intel_export_{timestamp}.json")
        
        result = f"‚úÖ **Data exported to JSON**\n\n" + \
                f"**File:** `{export_path.name}`\n" + \
                f"**Location:** `{export_path}`\n" + \
                f"**Records:** {len(findings)} competitors\n\n"
        
        if include_trends:
            trends = load_trends()
            trends_path = EXPORTS_DIR / f"trends_export_{timestamp}.json"
            with open(trends_path, 'w') as f:
                json.dump(trends, f, indent=2)
            result += f"**Trends File:** `{trends_path.name}`\n\n"
        
        result += "JSON format preserves full data structure and is suitable for programmatic analysis."
        
        return result

@FunctionTool
def view_reports_history(tool_context: "ToolContext", limit: int = 10) -> str:
    """
    View history of generated intelligence reports.
    
    Args:
        limit: Number of recent reports to show (default: 10, max: 50)
    
    Returns:
        List of recent reports with metadata and quick stats
    """
    limit = min(limit, 50)  # Cap at 50
    
    reports = sorted(
        REPORTS_DIR.glob("intel_report_*.md"),
        key=lambda p: p.stat().st_mtime,
        reverse=True
    )
    
    if not reports:
        return "üìö **No reports found**\n\n" + \
               "Run `run_intelligence_check` to generate your first competitive intelligence report."
    
    result = f"# üìö Report History\n\n"
    result += f"**Total Reports:** {len(reports)}\n\n"
    
    for i, report_path in enumerate(reports[:limit], 1):
        stat = report_path.stat()
        timestamp = datetime.fromtimestamp(stat.st_mtime)
        size_kb = stat.st_size / 1024
        
        # Try to extract key info from report
        try:
            with open(report_path, 'r', encoding='utf-8') as f:
                content = f.read()
                # Count competitors mentioned
                comp_count = content.count('###') - content.count('####')
                
        except:
            comp_count = "?"
        
        result += f"## {i}. {report_path.name}\n\n"
        result += f"- **Generated:** {timestamp.strftime('%B %d, %Y at %I:%M %p')}\n"
        result += f"- **Size:** {size_kb:.1f} KB\n"
        result += f"- **Competitors:** ~{comp_count}\n"
        result += f"- **Path:** `{report_path}`\n\n"
    
    if len(reports) > limit:
        result += f"\n*Showing {limit} of {len(reports)} reports. Increase limit parameter to see more.*\n"
    
    result += "\n---\n\n"
    result += "üí° **Tips:**\n"
    result += "- Reports are saved as Markdown files for easy reading\n"
    result += "- You can open them in any text editor or Markdown viewer\n"
    result += "- Reports include collapsible sections for detailed information\n"
    
    return result

@FunctionTool
def compare_competitors(
    competitor1: str,
    competitor2: str,
    tool_context: "ToolContext"
) -> str:
    """
    Compare two competitors side-by-side based on their trends and recent activity.
    
    Args:
        competitor1: Name of first competitor
        competitor2: Name of second competitor
    
    Returns:
        Side-by-side comparison analysis
    """
    trends1 = TrendAnalyzer.get_competitor_trends(competitor1)
    trends2 = TrendAnalyzer.get_competitor_trends(competitor2)
    
    if 'error' in trends1:
        return f"‚ùå No historical data for '{competitor1}'. Check the name or monitor them first."
    if 'error' in trends2:
        return f"‚ùå No historical data for '{competitor2}'. Check the name or monitor them first."
    
    report = f"# ‚öñÔ∏è Competitor Comparison\n\n"
    report += f"## {competitor1} vs {competitor2}\n\n"
    
    # Activity comparison
    report += "### Activity Levels\n\n"
    report += f"| Metric | {competitor1} | {competitor2} |\n"
    report += f"|--------|{'-'*len(competitor1)}--|{'-'*len(competitor2)}--|\n"
    report += f"| Total Updates | {trends1['total_updates']} | {trends2['total_updates']} |\n"
    report += f"| Recent Activity | {trends1['recent_activity']} | {trends2['recent_activity']} |\n"
    report += f"| Trend | {trends1['activity_trend'].title()} | {trends2['activity_trend'].title()} |\n"
    report += f"| Avg Impact | {trends1['average_impact']:.1f}/4.0 | {trends2['average_impact']:.1f}/4.0 |\n\n"
    
    # Impact distribution
    report += "### Impact Distribution\n\n"
    report += f"**{competitor1}:**\n"
    for impact, count in trends1['impact_distribution'].items():
        emoji = ReportGenerator._get_impact_emoji(impact)
        report += f"{emoji} {impact}: {count} "
    report += "\n\n"
    
    report += f"**{competitor2}:**\n"
    for impact, count in trends2['impact_distribution'].items():
        emoji = ReportGenerator._get_impact_emoji(impact)
        report += f"{emoji} {impact}: {count} "
    report += "\n\n"
    
    # Focus areas
    report += "### Recent Focus Areas\n\n"
    report += f"**{competitor1}:**\n"
    if trends1['recent_focus']:
        for keyword, count in trends1['recent_focus'][:5]:
            report += f"- {keyword} ({count}x)\n"
    else:
        report += "- No recent focus data\n"
    report += "\n"
    
    report += f"**{competitor2}:**\n"
    if trends2['recent_focus']:
        for keyword, count in trends2['recent_focus'][:5]:
            report += f"- {keyword} ({count}x)\n"
    else:
        report += "- No recent focus data\n"
    report += "\n"
    
    # Analysis
    report += "### Comparative Analysis\n\n"
    
    more_active = competitor1 if trends1['total_updates'] > trends2['total_updates'] else competitor2
    higher_impact = competitor1 if trends1['average_impact'] > trends2['average_impact'] else competitor2
    
    report += f"- **More Active:** {more_active}\n"
    report += f"- **Higher Average Impact:** {higher_impact}\n"
    
    if trends1['activity_trend'] == 'increasing' and trends2['activity_trend'] != 'increasing':
        report += f"- **Momentum:** {competitor1} is gaining momentum\n"
    elif trends2['activity_trend'] == 'increasing' and trends1['activity_trend'] != 'increasing':
        report += f"- **Momentum:** {competitor2} is gaining momentum\n"
    
    report += "\nüí° Use this comparison to prioritize which competitor requires closer monitoring."
    
    return report

@FunctionTool
def reset_cache(tool_context: "ToolContext", confirm: bool = False) -> str:
    """
    Reset the content cache to establish a new baseline.
    
    WARNING: All content will be considered "new" on the next check, which may result
    in a large number of updates being reported.
    
    Args:
        confirm: Set to True to confirm reset (safety check)
    
    Returns:
        Confirmation message
    """
    if not confirm:
        return "‚ö†Ô∏è **Confirm Cache Reset**\n\n" + \
               "This will clear the cache and treat all competitor content as new.\n\n" + \
               "**Impact:**\n" + \
               "- Next check may report many updates\n" + \
               "- Useful after long periods of inactivity\n" + \
               "- Historical trend data is preserved\n\n" + \
               "To confirm, use: `reset_cache(confirm=True)`"
    
    cache = load_cache()
    entries_count = len(cache)
    
    cache.clear()
    save_cache(cache)
    
    logger.info("Cache reset by user")
    
    return f"‚úÖ **Cache reset complete**\n\n" + \
           f"Cleared {entries_count} cache entries.\n\n" + \
           "All competitor content will be treated as new on the next intelligence check.\n" + \
           "Historical trend data has been preserved."

@FunctionTool
def get_status(tool_context: "ToolContext") -> str:
    """
    Get comprehensive system status, statistics, and health check.
    
    Returns:
        Detailed system status information
    """
    competitors = load_competitors()
    config = load_config()
    cache = load_cache()
    reports = list(REPORTS_DIR.glob("intel_report_*.md"))
    trends = load_trends()
    
    enabled_count = sum(1 for c in competitors if c.enabled)
    
    # Count by priority
    priority_counts = {'High': 0, 'Medium': 0, 'Low': 0}
    for comp in competitors:
        priority_counts[comp.priority] += 1
    
    status = "# üéØ IntelTracker System Status\n\n"
    status += f"**Version:** 12.0 Production Ready\n"
    status += f"**AI Engine:** {'‚úÖ Gemini 2.5 Flash' if GEMINI_AVAILABLE else '‚ö†Ô∏è Rule-based (Gemini unavailable)'}\n"
    status += f"**Status:** {'üü¢ Operational' if enabled_count > 0 else 'üü° No active competitors'}\n\n"
    
    status += "## üìä Monitoring Overview\n\n"
    status += f"- **Total Competitors:** {len(competitors)}\n"
    status += f"  - üî¥ High Priority: {priority_counts['High']}\n"
    status += f"  - üü° Medium Priority: {priority_counts['Medium']}\n"
    status += f"  - üü¢ Low Priority: {priority_counts['Low']}\n"
    status += f"- **Active Monitoring:** {enabled_count} competitors\n"
    status += f"- **Disabled:** {len(competitors) - enabled_count}\n"
    status += f"- **Cache Entries:** {len(cache)}\n"
    status += f"- **Trend Histories:** {len(trends)}\n\n"
    
    status += "## ‚öôÔ∏è Configuration\n\n"
    status += f"- **Check Interval:** {config.check_interval_hours} hours\n"
    status += f"- **Scheduled Monitoring:** {'‚úÖ Enabled' if config.enable_scheduling else '‚ùå Disabled'}\n"
    status += f"- **Report Format:** {config.digest_format.title()}\n"
    status += f"- **Max Updates/Competitor:** {config.max_updates_per_competitor}\n"
    status += f"- **Auto-Export:** {'‚úÖ Enabled' if config.export_after_check else '‚ùå Disabled'}\n"
    status += f"- **Rate Limiting:** {config.rate_limit_seconds}s between checks\n\n"
    
    status += "## üìß Notifications\n\n"
    if config.notification_email:
        status += f"- **Email:** {config.notification_email}\n"
        status += f"- **Critical Alerts:** {'‚úÖ' if config.notify_on_critical else '‚ùå'}\n"
        status += f"- **High Impact Alerts:** {'‚úÖ' if config.notify_on_high else '‚ùå'}\n"
    else:
        status += "- ‚ùå Not configured\n"
        status += "- Use `configure_monitoring` to set up email notifications\n"
    status += "\n"
    
    status += "## üìö Reports & Data\n\n"
    status += f"- **Total Reports:** {len(reports)}\n"
    
    if reports:
        latest = max(reports, key=lambda p: p.stat().st_mtime)
        latest_time = datetime.fromtimestamp(latest.stat().st_mtime)
        status += f"- **Latest Report:** {latest_time.strftime('%B %d at %I:%M %p')}\n"
        
        total_size = sum(r.stat().st_size for r in reports) / 1024
        status += f"- **Total Report Size:** {total_size:.1f} KB\n"
    
    if 'last_check' in tool_context.state:
        last_check = datetime.fromisoformat(tool_context.state['last_check'])
        time_since = datetime.now() - last_check
        
        status += f"- **Last Check:** {last_check.strftime('%B %d at %I:%M %p')}\n"
        status += f"- **Time Since:** {time_since.days}d {time_since.seconds//3600}h ago\n"
        status += f"- **Last Findings:** {tool_context.state.get('last_findings_count', 0)} updates\n"
    else:
        status += "- **Last Check:** Never (run `run_intelligence_check` to start)\n"
    
    status += "\n## üíæ Storage\n\n"
    status += f"- **Data Directory:** `{DATA_DIR}`\n"
    status += f"- **Reports Directory:** `{REPORTS_DIR}`\n"
    status += f"- **Exports Directory:** `{EXPORTS_DIR}`\n"
    status += f"- **Log File:** `{LOG_FILE}`\n\n"
    
    status += "## üîß Quick Actions\n\n"
    status += "- `run_intelligence_check()` - Check all competitors now\n"
    status += "- `list_competitors()` - View tracking list\n"
    status += "- `configure_monitoring()` - Adjust settings\n"
    status += "- `view_reports_history()` - Browse past reports\n"
    
    if not GEMINI_AVAILABLE:
        status += "\n‚ö†Ô∏è **Note:** Gemini API not available. Using rule-based analysis.\n"
        status += "Set `GEMINI_API_KEY` environment variable for AI-powered analysis."
    
    return status

# --- Agent Definition ---
root_agent = LlmAgent(
    name="intel_tracker",
    model="gemini-2.5-flash",
    instruction="""You are IntelTracker, an advanced competitive intelligence assistant designed for startups and businesses.

**Your Mission:**
Help businesses stay ahead by monitoring competitors, analyzing their moves, and providing actionable strategic insights.

**Core Capabilities:**
- üîç Monitor competitor websites, RSS feeds, social media, and press releases
- ü§ñ AI-powered analysis using Gemini 2.5 Flash (when available)
- üìä Track trends and patterns over time
- üìß Send email alerts for critical updates
- üìà Generate intelligence reports in multiple formats
- üíæ Export data for further analysis

**Primary Workflows:**

1. **Daily Monitoring:**
   - User: "Check competitors"
   - You: Run `run_intelligence_check()` and summarize key findings

2. **Adding Competitors:**
   - User: "Track [Company]"
   - You: Use `add_competitor()` with available info (website, RSS, etc.)

3. **Strategic Analysis:**
   - User: "Compare [A] and [B]"
   - You: Use `compare_competitors()` and provide strategic insights

4. **Trend Spotting:**
   - User: "What's [Company] been up to?"
   - You: Use `view_competitor_trends()` to show patterns

**Communication Style:**
- Be professional yet conversational
- Focus on actionable insights, not just data dumps
- Highlight business implications ("This matters because...")
- Prioritize high-impact information
- Use clear categorization (Critical/High/Medium/Low)
- Provide specific, tactical recommendations

**When Analyzing Updates:**
1. Assess business impact and competitive threat
2. Identify strategic implications
3. Spot opportunities for your user
4. Recommend specific actions
5. Consider market context and trends

**Proactive Suggestions:**
- If no recent updates: Suggest checking individual competitors or adjusting monitoring frequency
- If many updates: Offer to focus on high-priority competitors
- If trends show patterns: Point them out and explain significance
- If email not configured: Suggest setting it up for alerts

**Error Handling:**
- If extraction fails: Explain clearly and suggest alternatives (RSS feed, manual review)
- If API unavailable: Note that you're using rule-based analysis
- If competitor not found: Suggest similar names or offer to add them

**Remember:**
- Your goal is to save time and provide strategic advantage
- Every insight should be actionable
- Context matters - explain why changes are important
- Be the user's competitive intelligence analyst, not just a data collector

**Available Tools:**
- run_intelligence_check - Main monitoring function
- add/remove/toggle_competitor - Management
- check_single_competitor - Focus on one
- view_competitor_trends - Historical analysis
- compare_competitors - Side-by-side analysis
- configure_monitoring - Settings
- export_intelligence_data - Data export
- view_reports_history - Past reports
- reset_cache - Fresh start
- get_status - System overview""",
    tools=[
        run_intelligence_check,
        add_competitor,
        list_competitors,
        remove_competitor,
        toggle_competitor,
        check_single_competitor,
        view_competitor_trends,
        compare_competitors,
        configure_monitoring,
        export_intelligence_data,
        view_reports_history,
        reset_cache,
        get_status
    ],
)

# --- Scheduled Monitoring (Optional) ---
def scheduled_check():
    """Run scheduled intelligence check."""
    logger.info("Running scheduled check...")
    try:
        engine = MonitoringEngine()
        findings = engine.monitor_all()
        
        if findings:
            config = load_config()
            report = ReportGenerator.generate_digest(findings, config.digest_format)
            ReportGenerator.save_report(report)
            
            # Send email if configured
            if config.notification_email:
                notifier = EmailNotifier(config)
                notifier.send_digest(report, len(findings))
            
            logger.info(f"Scheduled check complete: {len(findings)} findings")
        else:
            logger.info("Scheduled check complete: No new findings")
            
    except Exception as e:
        logger.error(f"Scheduled check failed: {e}", exc_info=True)

def start_scheduler():
    """Start background scheduler if enabled."""
    config = load_config()
    
    if not config.enable_scheduling:
        return
    
    logger.info(f"Starting scheduler: checks every {config.check_interval_hours} hours")
    
    schedule.every(config.check_interval_hours).hours.do(scheduled_check)
    
    def run_scheduler():
        while True:
            schedule.run_pending()
            time.sleep(60)  # Check every minute
    
    scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
    scheduler_thread.start()

# --- Main Execution ---
if __name__ == "__main__":
    print("=" * 70)
    print("üéØ IntelTracker v12.0 - Production-Ready Competitive Intelligence")
    print("=" * 70)
    print()
    
    # Initialize
    config = load_config()
    competitors = load_competitors()
    
    # Status overview
    enabled_count = sum(1 for c in competitors if c.enabled)
    print(f"üìä Tracking: {len(competitors)} competitors ({enabled_count} active)")
    print(f"‚öôÔ∏è  AI Engine: {'Gemini 2.5 Flash ‚úÖ' if GEMINI_AVAILABLE else 'Rule-based ‚ö†Ô∏è'}")
    print(f"üìù Report Format: {config.digest_format.title()}")
    
    if config.notification_email:
        print(f"üìß Notifications: {config.notification_email}")
    
    print()
    print("-" * 70)
    print()
    
    # Run intelligence check
    print("üîç Running intelligence check...\n")
    
    try:
        engine = MonitoringEngine()
        findings = engine.monitor_all()
        
        if findings:
            print()
            print("=" * 70)
            print(f"üìä Found {len(findings)} competitor update(s)")
            print("=" * 70)
            print()
            
            # Generate and save report
            report = ReportGenerator.generate_digest(findings, config.digest_format)
            report_path = ReportGenerator.save_report(report)
            
            # Export if configured
            if config.export_after_check:
                ExportManager.export_to_json(findings)
                print(f"üíæ Exported to JSON")
            
            # Display report
            print(report)
            print()
            print("=" * 70)
            print(f"‚úÖ Report saved: {report_path}")
            
            # Send email if configured
            if config.notification_email:
                notifier = EmailNotifier(config)
                if notifier.send_digest(report, len(findings)):
                    print(f"üìß Report emailed to {config.notification_email}")
                else:
                    print("‚ö†Ô∏è  Email sending failed")
        else:
            print("‚úÖ No new updates detected")
            print()
            print("All competitors have been checked. No new activity since last check.")
        
        print()
        print("-" * 70)
        
        # Start scheduler if enabled
        if config.enable_scheduling:
            print(f"\n‚è∞ Scheduler enabled: Will check every {config.check_interval_hours} hours")
            print("Press Ctrl+C to stop...\n")
            start_scheduler()
            
            try:
                while True:
                    time.sleep(1)
            except KeyboardInterrupt:
                print("\n\nüëã Scheduler stopped")
        else:
            print("\nüí° Tip: Use ADK to interact conversationally with IntelTracker")
            print("   Or enable scheduling with: configure_monitoring(enable_scheduling=True)")
        
    except KeyboardInterrupt:
        print("\n\nüëã Exiting IntelTracker")
    except Exception as e:
        logger.error(f"Fatal error: {e}", exc_info=True)
        print(f"\n‚ùå Error: {e}")
        print("Check the log file for details: {LOG_FILE}")
    
    print("\n" + "=" * 70)
    print("üöÄ IntelTracker ready for use!")
    print("=" * 70)
